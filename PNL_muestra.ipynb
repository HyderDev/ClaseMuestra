{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraciones: ['El gato está durmiendo en la alfombra.', 'Mi nombre es Juan y vivo en Nueva York.']\n",
      "Tokens: ['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra', '.', 'Mi', 'nombre', 'es', 'Juan', 'y', 'vivo', 'en', 'Nueva', 'York', '.']\n",
      "Etiquetas gramaticales: [('El', 'NNP'), ('gato', 'NN'), ('está', 'NN'), ('durmiendo', 'NN'), ('en', 'IN'), ('la', 'NN'), ('alfombra', 'NN'), ('.', '.'), ('Mi', 'NNP'), ('nombre', 'JJ'), ('es', 'NN'), ('Juan', 'NNP'), ('y', 'NN'), ('vivo', 'NN'), ('en', 'IN'), ('Nueva', 'NNP'), ('York', 'NNP'), ('.', '.')]\n",
      "Entidades nombradas: (S\n",
      "  (GPE El/NNP)\n",
      "  gato/NN\n",
      "  está/NN\n",
      "  durmiendo/NN\n",
      "  en/IN\n",
      "  la/NN\n",
      "  alfombra/NN\n",
      "  ./.\n",
      "  Mi/NNP\n",
      "  nombre/JJ\n",
      "  es/NN\n",
      "  (PERSON Juan/NNP)\n",
      "  y/NN\n",
      "  vivo/NN\n",
      "  en/IN\n",
      "  (PERSON Nueva/NNP York/NNP)\n",
      "  ./.)\n",
      "Palabras filtradas: ['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra', '.', 'Mi', 'nombre', 'es', 'Juan', 'vivo', 'en', 'Nueva', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    # Tokenización de oraciones\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"Oraciones:\", sentences)\n",
    "\n",
    "    # Tokenización de palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Etiquetado gramatical\n",
    "    tagged = pos_tag(tokens)\n",
    "    print(\"Etiquetas gramaticales:\", tagged)\n",
    "\n",
    "    # Reconocimiento de entidades nombradas\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    print(\"Entidades nombradas:\", named_entities)\n",
    "\n",
    "    # Eliminación de palabras vacías (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [token for token in tokens if token.lower() not in stop_words]\n",
    "    print(\"Palabras filtradas:\", filtered_words)\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto_ejemplo = \"El gato está durmiendo en la alfombra. Mi nombre es Juan y vivo en Nueva York.\"\n",
    "\n",
    "# Procesar el texto\n",
    "process_text(texto_ejemplo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuesta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que verificar que cumplimos con los requerimientos del sistema, mediante la instalación del módulo NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.10.4/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.4/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.4/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se instalan los modelos pre-entrenados para que funcione el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se descarga el modelo para la división de texto\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera que en el primer ejercicio, lo que tenemos que realizar es la importación de los módulos que vamos a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para poder leer nuestro texto desde un archivo txt tenemos que realizar lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos dentro del archivo 'texto.txt': \n",
      "El gato está durmiendo en la alfombra. Mi nombre es Juan y vivo en Nueva York.\n"
     ]
    }
   ],
   "source": [
    "# Se usa 'with open' ya que nos facilita el hecho de no tener que cerrar \n",
    "# el archivo en códigos grandes como ocurre al usar solamente 'open'.\n",
    "with open('texto.txt', 'r') as texto: # Se señala que solo buscamos leer el archivo con 'r' \n",
    "    texto_ejemplo = texto.read()\n",
    "\n",
    "\n",
    "# En este caso usamos \" para poder ingresar ' dentro del texto impreso.\n",
    "print(f\"Datos dentro del archivo 'texto.txt': \\n{texto_ejemplo}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función a usar, que básicamente sería la mimsa que ya tenemos y la corremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraciones: ['El gato está durmiendo en la alfombra.', 'Mi nombre es Juan y vivo en Nueva York.']\n",
      "Tokens: ['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra', '.', 'Mi', 'nombre', 'es', 'Juan', 'y', 'vivo', 'en', 'Nueva', 'York', '.']\n",
      "Etiquetas gramaticales: [('El', 'NNP'), ('gato', 'NN'), ('está', 'NN'), ('durmiendo', 'NN'), ('en', 'IN'), ('la', 'NN'), ('alfombra', 'NN'), ('.', '.'), ('Mi', 'NNP'), ('nombre', 'JJ'), ('es', 'NN'), ('Juan', 'NNP'), ('y', 'NN'), ('vivo', 'NN'), ('en', 'IN'), ('Nueva', 'NNP'), ('York', 'NNP'), ('.', '.')]\n",
      "Entidades nombradas: (S\n",
      "  (GPE El/NNP)\n",
      "  gato/NN\n",
      "  está/NN\n",
      "  durmiendo/NN\n",
      "  en/IN\n",
      "  la/NN\n",
      "  alfombra/NN\n",
      "  ./.\n",
      "  Mi/NNP\n",
      "  nombre/JJ\n",
      "  es/NN\n",
      "  (PERSON Juan/NNP)\n",
      "  y/NN\n",
      "  vivo/NN\n",
      "  en/IN\n",
      "  (PERSON Nueva/NNP York/NNP)\n",
      "  ./.)\n",
      "Palabras filtradas: ['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra', '.', 'Mi', 'nombre', 'es', 'Juan', 'vivo', 'en', 'Nueva', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    # Tokenización de oraciones\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"Oraciones:\", sentences)\n",
    "\n",
    "    # Tokenización de palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Etiquetado gramatical\n",
    "    tagged = pos_tag(tokens)\n",
    "    print(\"Etiquetas gramaticales:\", tagged)\n",
    "\n",
    "    # Reconocimiento de entidades nombradas\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    print(\"Entidades nombradas:\", named_entities)\n",
    "\n",
    "    # Eliminación de palabras vacías (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [token for token in tokens if token.lower() not in stop_words]\n",
    "    print(\"Palabras filtradas:\", filtered_words)\n",
    "\n",
    "\n",
    "# Procesar el texto\n",
    "process_text(texto_ejemplo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
